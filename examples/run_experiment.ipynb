{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from athina.evals import ContextContainsEnoughInformation\n",
    "from athina.loaders import RagLoader\n",
    "from athina.keys import AthinaApiKey, OpenAiApiKey\n",
    "from athina.interfaces.athina import AthinaExperiment\n",
    "from athina.datasets import yc_query_mini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure your API keys\n",
    "\n",
    "Evals use OpenAI, so you need to configure your OpenAI API key.\n",
    "\n",
    "If you wish to view the results on Athina's UI, and maintain a historical record of experiments, then you also need an Athina API Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OpenAiApiKey.set_key(os.getenv('OPENAI_API_KEY'))\n",
    "AthinaApiKey.set_key(os.getenv('ATHINA_API_KEY')) # Optional, recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your dataset\n",
    "\n",
    "You can use one of our `loaders` to load the data from a Dictionary, CSV or JSON file.\n",
    "\n",
    "Here's an example\n",
    "```\n",
    "from athina.loaders import RagLoader\n",
    "\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "```\n",
    "\n",
    "Here is the complete [documentation](https://docs.athina.ai/evals/running_evals/loading_data) specifying the various ways you can load your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load batch dataset\n",
    "raw_data = yc_query_mini.data\n",
    "dataset = RagLoader().load_dict(raw_data)\n",
    "\n",
    "pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe your experiment metadata fields (optional)\n",
    "These metadata fields are only used as identifiers when we save your experiment on Athina Develop.\n",
    "This helps you search, sort and filter through past experimentation runs.\n",
    "\n",
    "Currently, this includes your:\n",
    "- `experiment_name`: (string) The name of your experiment\n",
    "- `experiment_description`: (string) A description this iteration of your experiment\n",
    "- `language_model_provider`: (string) `openai`\n",
    "- `language_model_id`: (string) The language model used for the LLM inference (ex: `gpt-3.5-turbo`)\n",
    "- `prompt_template`: (object) A JS object representing the prompt you are sending to the LLM (for example, messages array in OpenAI)\n",
    "- `dataset_name`: (string) An identifier for the dataset you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your experiment parameters\n",
    "prompt_template = [\n",
    "    { \n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an expert at answering questions about Y Combinator. If you do not know the answer, say I don't know. Be direct and concise in your responses\" },\n",
    "    { \n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"{query}\"\n",
    "    }\n",
    "]\n",
    "experiment = AthinaExperiment(\n",
    "    experiment_name=\"ContextRelevance\",\n",
    "    experiment_description=\"Checking retrieval scores for YC dataset with a simple zero-shot prompt\",\n",
    "    language_model_provider=\"openai\",\n",
    "    language_model_id=\"gpt-3.5-turbo\",\n",
    "    prompt_template=prompt_template,\n",
    "    dataset_name=\"yc_dataset_mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your evaluation\n",
    "\n",
    "Simply instantiate the evaluator class you wish to use, and call `run_batch` to the eval\n",
    "\n",
    "##### Run evals in parallel (much faster)\n",
    "\n",
    "You may specify `max_parallel_evals` to run multiple LLM evaluation inferences in parallel.\n",
    "\n",
    "##### View as a dataframe\n",
    "Call `.to_df()` on the results to view as a dataframe\n",
    "\n",
    "\n",
    "##### Log results to Athina Develop (Dashboard UI)\n",
    "If you have specified an `AthinaApiKey`, then results will automatically logged to the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the LLM response answers the user query sufficiently\n",
    "results = ContextContainsEnoughInformation().configure_experiment(experiment).run_batch(\n",
    "    data=dataset,\n",
    "    max_parallel_evals=5 # Run up to 5 evals in parallel\n",
    ")\n",
    "\n",
    "results.to_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
